{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practica2-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_mvGMFt46Lr0","colab_type":"code","colab":{}},"source":["# Alumno : Daniel Bolaños Martínez 76592621E\n","# Asignatura : Visión por Computador\n","# Práctica 2 : Introducción a Keras para la clasificación de imágenes\n","\n","# -*- coding: utf-8 -*-\n","\n","! pip install -q keras"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOYa5wIm6VEu","colab_type":"code","colab":{}},"source":["#########################################################################\n","############ CARGAR LAS LIBRERÍAS NECESARIAS ############################\n","#########################################################################\n","\n","# Importar librerías necesarias\n","import numpy as np\n","import keras\n","import matplotlib.pyplot as plt\n","import keras.utils as np_utils\n","\n","# Importar modelos y capas que se van a usar\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Activation\n","from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n","from keras import backend as K\n","# Import Early Stopping\n","from keras.callbacks import EarlyStopping\n","\n","# Importar el optimizador a usar\n","from keras.optimizers import SGD\n","# Importar el conjunto de datos\n","from keras.datasets import cifar100\n","# Importar DataGenerator\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8USt6LhGuno","colab_type":"text"},"source":["####Dimensiones constantes"]},{"cell_type":"code","metadata":{"id":"s-XFtrV3Gteb","colab_type":"code","colab":{}},"source":["# input image dimensions\n","img_rows, img_cols = 32, 32\n","num_classes = 25\n","batch_size = 32\n","epochs = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tlv0hxT37t2u","colab_type":"code","colab":{}},"source":["#########################################################################\n","######## FUNCIÓN PARA CARGAR Y MODIFICAR EL CONJUNTO DE DATOS ###########\n","#########################################################################\n","\n","# A esta función sólo se le llama una vez. Devulve 4\n","# vectores conteniendo, por este orden, las imágenes\n","# de entrenamiento, las clases de las imágenes de\n","# entrenamiento, las im-agenes del conjunto de test y\n","# las clases del conjunto de test.\n","\n","def cargarImagenes():\n","  # Cargamos Cifar100. Cada imagen tiene tamaño\n","  # (32, 32, 3). Nos vamos a quedar con las\n","  # imágenes de 25 de las clases.\n","  (x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n","  x_train = x_train.astype('float32')\n","  x_test = x_test.astype('float32')\n","  \n","  x_train /= 255\n","  x_test /= 255\n","  \n","  train_idx = np.isin(y_train, np.arange(25))\n","  train_idx = np.reshape(train_idx, -1)\n","  x_train = x_train[train_idx]\n","  y_train = y_train[train_idx]\n","  \n","  test_idx = np.isin(y_test, np.arange(25))\n","  test_idx = np.reshape(test_idx, -1)\n","  x_test = x_test[test_idx]\n","  y_test = y_test[test_idx]\n","\n","  # Transformamos los vectores de clases en matrices.\n","  # Cada componente se convierte en un vector de ceros\n","  # con un uno en la componente correspondiente a la\n","  # clase a la que pertenece la imagen. Este paso es\n","  # necesario para la clasificación multiclase en keras.\n","  \n","  y_train = np_utils.to_categorical(y_train, 25)\n","  y_test = np_utils.to_categorical(y_test, 25) \n","\n","  return x_train, y_train, x_test, y_test\n","\n","#########################################################################\n","######## FUNCIÓN PARA OBTENER EL ACCURACY DEL CONJUNTO DE TEST ##########\n","#########################################################################\n","\n","# Esta función devuelve el accuracy de un modelo, defi-\n","# nido como el porcentaje de etiquetas bien predichas\n","# frente al total de etiquetas. Como parámetros es\n","# necesario pasarle el vector de etiquetas verdaderas\n","# y el vector de etiquetas predichas, en el formato de\n","# keras (matrices donde cada etiqueta ocupa una fila,\n","# con un 1 en la posición de la clase a la que pertenece\n","# 0 en las demás).\n","\n","def calcularAccuracy(labels, preds):\n","  labels = np.argmax(labels, axis=1)\n","  preds = np.argmax(preds, axis=1)\n","  \n","  accuracy = sum(labels == preds)/len(labels)\n","  \n","  return accuracy\n","\n","#########################################################################\n","## FUNCIÓN PARA PINTAR LA PÉRDIDA Y EL ACCURACY EN TRAIN Y VALIDACIÓN ###\n","#########################################################################\n","\n","# Esta función pinta dos gráficas, una con la evolución\n","# de la función de pérdida en el conjunto de train y\n","# en el de validación, y otra con la evolución del\n","# accuracy en el conjunto de train y el de validación.\n","# Es necesario pasarle como parámetro el historial del\n","# entrenamiento del modelo (lo que devuelven las \n","# funciones fit() y fit_generator()))).\n","\n","def mostrarEvolucion(hist):\n","  loss = hist.history['loss']\n","  val_loss = hist.history['val_loss']\n","  plt.plot(loss)\n","  plt.plot(val_loss)\n","  plt.legend(['Training loss', 'Validation loss'])\n","  plt.show()\n","\n","  acc = hist.history['acc']\n","  val_acc = hist.history['val_acc']\n","  plt.plot(acc)\n","  plt.plot(val_acc)\n","  plt.legend(['Training accuracy', 'Validation accuracy'])\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8ieGGyU9_A9","colab_type":"text"},"source":["###Apartado 1: BaseNet en CIFAR100\n"]},{"cell_type":"code","metadata":{"id":"LNORpslK6iLP","colab_type":"code","colab":{}},"source":["#########################################################################\n","################## DEFINICIÓN DEL MODELO BASENET ########################\n","#########################################################################\n","\n","x_train, y_train, x_test, y_test = cargarImagenes()\n","\n","input_shape = (img_rows, img_cols, 3)\n","\n","model = Sequential()\n","model.add(Conv2D(6, kernel_size=(5,5), activation='relu',\n","                 input_shape=input_shape))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Conv2D(16, kernel_size=(5,5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","model.add(Flatten())\n","model.add(Dense(50, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3ZFd0sS7naR","colab_type":"code","colab":{}},"source":["#########################################################################\n","######### DEFINICIÓN DEL OPTIMIZADOR Y COMPILACIÓN DEL MODELO ###########\n","#########################################################################\n","\n","# explicar por qué selecciono ese optimizador\n","\n","opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=opt,\n","              metrics=['acc'])\n","\n","# Una vez tenemos el modelo base, y antes de entrenar, vamos a guardar los\n","# pesos aleatorios con los que empieza la red, para poder reestablecerlos\n","# después y comparar resultados entre no usar mejoras y sí usarlas.\n","\n","weights = model.get_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"It9dQUln6n27","colab_type":"code","colab":{}},"source":["#########################################################################\n","###################### ENTRENAMIENTO DEL MODELO #########################\n","#########################################################################\n","\n","datagen_train = ImageDataGenerator(validation_split = 0.1)\n","train = datagen_train.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation = datagen_train.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = model.fit_generator(train, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation, validation_steps = len(x_train)*0.1/batch_size)\n","\n","mostrarEvolucion(histograma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IVvOuDyf9JPb","colab_type":"code","colab":{}},"source":["#########################################################################\n","################ PREDICCIÓN SOBRE EL CONJUNTO DE TEST ###################\n","#########################################################################\n","\n","preds = model.predict(x_test)\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = model.predict(x_train)\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3F1LT_W--N3H","colab_type":"text"},"source":["###Apartado 2: Mejora del modelo"]},{"cell_type":"markdown","metadata":{"id":"t8-egVsN7oTJ","colab_type":"text"},"source":["####Normalización"]},{"cell_type":"code","metadata":{"id":"Xh5NMQoV9K-S","colab_type":"code","colab":{}},"source":["#########################################################################\n","########################## MEJORA DEL MODELO ############################\n","########################### NORMALIZACIÓN ###############################\n","#########################################################################\n","\n","model.set_weights(weights)\n","\n","datagen_train_norm = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True, validation_split = 0.1)\n","datagen_test_norm = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n","datagen_train_norm.fit(x_train)\n","datagen_test_norm.fit(x_train)\n","train_norm = datagen_train_norm.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation_norm = datagen_train_norm.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = model.fit_generator(train_norm, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation_norm, validation_steps = len(x_train)*0.1/batch_size)\n","\n","mostrarEvolucion(histograma)\n","\n","preds = model.predict_generator(datagen_test_norm.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = model.predict_generator(datagen_train_norm.flow(x_train, batch_size = 1, shuffle = False), steps = len(x_train))\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFC2Os4J7ww2","colab_type":"text"},"source":["####Aumento de datos"]},{"cell_type":"code","metadata":{"id":"FFG_Z30Z71p5","colab_type":"code","colab":{}},"source":["#########################################################################\n","########################## MEJORA DEL MODELO ############################\n","########################## AUMENTO DE DATOS #############################\n","#########################################################################\n","\n","# Probar diversos parámetros para el aumento, sin flip, cambiar zoom_range...\n","# Justificar por qué puedo hacer flip (no cambio de la distribución en nuestro caso)\n","\n","model.set_weights(weights)\n","\n","datagen_train_zoom = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True,\n","                                        horizontal_flip=False, zoom_range=0.5, validation_split = 0.1)\n","datagen_test_zoom = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n","\n","datagen_train_zoom.fit(x_train)\n","datagen_test_zoom.fit(x_train)\n","\n","train_zoom = datagen_train_zoom.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation_zoom = datagen_train_zoom.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = model.fit_generator(train_zoom, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation_zoom, validation_steps = len(x_train)*0.1/batch_size)\n","\n","mostrarEvolucion(histograma)\n","\n","preds = model.predict_generator(datagen_test_zoom.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = model.predict_generator(datagen_train_zoom.flow(x_train, batch_size = 1, shuffle = False), steps = len(x_train))\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rWFPon8oRRN","colab_type":"text"},"source":["####Red más profunda"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pXvT_F80w4BE","colab":{}},"source":["# Crearemos un nuevo modelo añadiendo las capas que veamos necesarias\n","\n","x_train, y_train, x_test, y_test = cargarImagenes()\n","\n","input_shape = (img_rows, img_cols, 3)\n","\n","my_model = Sequential()\n","my_model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape=input_shape))\n","my_model.add(Dropout(0.25))\n","my_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n","my_model.add(MaxPooling2D(pool_size=(2,2)))\n","my_model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n","my_model.add(Dropout(0.25))\n","my_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n","my_model.add(MaxPooling2D(pool_size=(2,2)))\n","my_model.add(Flatten())\n","my_model.add(Dense(512, activation='relu'))\n","my_model.add(Dropout(0.5))\n","my_model.add(Dense(256, activation='relu'))\n","my_model.add(Dropout(0.5))\n","my_model.add(Dense(num_classes, activation='softmax'))\n","\n","opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","\n","my_model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=opt,\n","              metrics=['acc'])\n","\n","weights = my_model.get_weights()\n","\n","# Aplicamos la normalización y un aumento de datos usando los parámetros que mejor resultado han obtenido\n","\n","datagen_train_depth = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True, validation_split = 0.1)\n","datagen_test_depth = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n","\n","datagen_train_depth.fit(x_train)\n","datagen_test_depth.fit(x_train)\n","\n","train_depth = datagen_train_depth.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation_depth = datagen_train_depth.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = my_model.fit_generator(train_depth, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation_depth, validation_steps = len(x_train)*0.1/batch_size)\n","\n","mostrarEvolucion(histograma)\n","\n","preds = my_model.predict_generator(datagen_test_depth.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = my_model.predict_generator(datagen_train_depth.flow(x_train, batch_size = 1, shuffle = False), steps = len(x_train))\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LaOzFjyMiFro","colab_type":"text"},"source":["####Capas de normalización"]},{"cell_type":"code","metadata":{"id":"-dpWU9Ghi7Qu","colab_type":"code","colab":{}},"source":["# Crearemos un nuevo modelo añadiendo las capas que veamos necesarias\n","\n","x_train, y_train, x_test, y_test = cargarImagenes()\n","\n","input_shape = (img_rows, img_cols, 3)\n","\n","# Probar a insertar BatchNormalization antes y después de las capas Dense\n","\n","my_model2 = Sequential()\n","my_model2.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(Dropout(0.25))\n","my_model2.add(Conv2D(64, kernel_size=(3,3)))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(MaxPooling2D(pool_size=(2,2)))\n","my_model2.add(Conv2D(64, kernel_size=(3,3)))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(Dropout(0.25))\n","my_model2.add(Conv2D(128, kernel_size=(3,3)))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(MaxPooling2D(pool_size=(2,2)))\n","my_model2.add(Flatten())\n","my_model2.add(Dense(512))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(Dropout(0.5))\n","my_model2.add(Dense(256))\n","my_model2.add(BatchNormalization())\n","my_model2.add(Activation('relu'))\n","my_model2.add(Dropout(0.5))\n","my_model2.add(Dense(num_classes, activation='softmax'))\n","\n","opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","\n","my_model2.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=opt,\n","              metrics=['acc'])\n","\n","weights = my_model2.get_weights()\n","\n","# Aplicamos la normalización y un aumento de datos usando los parámetros que mejor resultado han obtenido\n","\n","datagen_train_final = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True, validation_split = 0.1)\n","datagen_test_final = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n","\n","datagen_train_final.fit(x_train)\n","datagen_test_final.fit(x_train)\n","\n","train_final = datagen_train_final.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation_final = datagen_train_final.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = my_model2.fit_generator(train_final, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation_final, validation_steps = len(x_train)*0.1/batch_size)\n","\n","mostrarEvolucion(histograma)\n","\n","preds = my_model2.predict_generator(datagen_test_final.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = my_model2.predict_generator(datagen_train_final.flow(x_train, batch_size = 1, shuffle = False), steps = len(x_train))\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QAtVBCTRlWka","colab_type":"text"},"source":["####Early Stopping"]},{"cell_type":"code","metadata":{"id":"IRbnG1xeldH5","colab_type":"code","colab":{}},"source":["# paramos cuando la gráfica de pérdida de validation comience a crecer\n","# y el valor accuracy de la validation se estabilice o decrezca\n","\n","# probamos con 40 y buscamos el punto de parada\n","\n","epochs = 40\n","\n","my_model2.set_weights(weights)\n","\n","datagen_train_final = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True, \n","                                         horizontal_flip=True, zoom_range=0.2, validation_split = 0.1)\n","datagen_test_final = ImageDataGenerator(featurewise_center = True, featurewise_std_normalization = True)\n","\n","datagen_train_final.fit(x_train)\n","datagen_test_final.fit(x_train)\n","\n","train_final = datagen_train_final.flow(x_train, y_train, batch_size = batch_size, subset = 'training')\n","validation_final = datagen_train_final.flow(x_train, y_train, batch_size = batch_size, subset = 'validation')\n","\n","histograma = my_model2.fit_generator(train_final, steps_per_epoch = len(x_train)*0.9/batch_size, epochs = epochs, \n","                    validation_data = validation_final, validation_steps = len(x_train)*0.1/batch_size,\n","                    callbacks = [EarlyStopping(monitor = 'val_acc', patience = 4, restore_best_weights = True)])\n","\n","mostrarEvolucion(histograma)\n","\n","preds = my_model2.predict_generator(datagen_test_final.flow(x_test, batch_size = 1, shuffle = False), steps = len(x_test))\n","score = calcularAccuracy(y_test, preds)\n","print(\"Predicción sobre conjunto Test\")\n","print(\"Test accuracy = \" + str(score))\n","# Predicción sobre el conjunto de Train\n","preds = my_model2.predict_generator(datagen_train_final.flow(x_train, batch_size = 1, shuffle = False), steps = len(x_train))\n","score = calcularAccuracy(y_train, preds)\n","print(\"Predicción sobre conjunto Train\")\n","print(\"Train accuracy = \" + str(score))"],"execution_count":0,"outputs":[]}]}